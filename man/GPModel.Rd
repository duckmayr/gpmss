% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/GPModel.R
\name{GPModel}
\alias{GPModel}
\title{Gaussian Process Model}
\description{
GPModel provides a parent class for all Gaussian process (GP) model classes
in \code{gpmss}. A GP model class keeps track of the data and various
posterior quantities, as well as the inference method and likelihood,
mean, and covariance functions. It has a number of methods to produce,
summarize, or plot quantities of interest such as predictive distributions
or avarage marginal effects.
}
\details{
In Gaussian process (GP) regression, we consider that our outcomes \eqn{y}
are noisy observations of an unknown function of the predictors \eqn{X}:

\deqn{
    y = f \left( X \right) + \varepsilon.
}{%
    y = f ( X ) + \varepsilon.
}

We place a GP prior over the values of \eqn{f \left( X \right)}{f(X)},
which says that before observing \eqn{y}, we believe the values of
\eqn{f \left( X \right)}{f(X)} are distributed

\deqn{
    f \left( X \right)
    \sim
    \mathcal{N} \left(
        m \left( X \right),
        K \left( X, X \right)
    \right),
}{%
    f ( X ) \sim N ( m ( X ), K ( X, X ) ),
}

where \eqn{m \left( X \right)}{m ( X )} is a \link[=MeanFunction]{mean function}
and \eqn{K \left( X, X \right)}{K ( X, X )} is a
\link[=CovarianceFunction]{covariance function}.
In other words, the prior mean over function outputs is a function of the
predictors, as is the prior covariance over function outputs.
Application of Bayes' rule in conjunction with Gaussian identities gives
us the posterior distribution of the values of \eqn{f\left(X\right)}{f(X)}
after observing \eqn{y}. This is a flexible Bayesian non-parametric
framework to reason about the relationship between predictors and outcomes
without making strong functional form assumptions.

GP classification works similarly, though now \eqn{f\left(X\right)}{f(X)}
is a latent function that is pushed through a link function to produce
outcomes, not unlike generalized linear models. While the exact posterior
for GP regression can be derived analytically, in the classification case
the posterior is analytically untractable, so several approximations to
the posterior are available, and the posterior distribution can be
simulated via MCMC methods.

A GPModel child class implements one such inference method for GP
regression or classification. The GP models already implemented by
\code{gpmss} are

\describe{
\item{GPR}{
Exact inference for GP regression.
This is only possible with a \link[=LikGauss]{Gaussian likelihood}.
}
\item{GPCLA}{
The Laplace approximation to the posterior for GP classification.
}
}

Users may also define their own GP models that utilize gpmss functions.
A GPModel subclass should provide a \code{train()} method,
a \code{predict()} method, a \code{nlml()} method, a \code{dnlml()}
method, and a \code{margins()} method.
A subclass may wish to provide a custome \code{optimize()} method for
optimizing hyperparameters, though the parent class' \code{optimize()}
method should be general purpose enough to work for any subclass designed
according to the description provided here.
It should have data members \code{name}, \code{y}, \code{X},
\code{meanfun} (to hold an instance of a subclass of \link{MeanFunction}),
\code{covfun} (to hold an instance of a subclass of \link{CovarianceFunction}),
\code{likfun} (to hold an instance of a subclass of \link{LikelihoodFunction}),
\code{marginal_effects}, and \code{average_marginal_effects},
but which other data members are appropriate may vary by inference method.
For most models, a data member \code{L} should be present,
which is a (lower) Cholesky decomposition of a matrix of central importance
in posterior inference, as well as \code{alpha}, which is usually
\eqn{K^{-1} \left(\mu_{post} - \mu_{prior}\right)}{%
     K^{-1} (\mu_{post} - \mu_{prior})},
where \eqn{\mu} is the posterior or prior mean of the function of interest
respectively.
But, again, one should consult specific models' help files
as the data members that are useful will differ between models.

The \code{train()} method is used to characterize the posterior distribution
of the function of interest at the training cases provided in the
\code{data} argument of the constructor
(or found in the calling environment given the formula provided).
For many models, arguments will not be needed; the quantities necessary for
inference should be provided in the constructor. However, an ellipses
(\code{...}) argument should always conclude the argument list.

The \code{predict()} method is used to characterize the posterior
predictive distribution of the function of interest at new test cases.
The first argument should be \code{newdata},
a data frame containing the data for the new test cases.
For many models, these may be the only arguments needed,
but additional arguments may be appropriate.
An ellipses (\code{...}) argument should always conclude the argument list.

The \code{nlml()} method is used to calculate the negative log marginal
likelihood.
For many models, arguments will not be needed; the quantities necessary for
inference should be provided in the constructor or generated during the
call to \code{train()}. However, an ellipses
(\code{...}) argument should always conclude the argument list.

The \code{dnlml()} method is used to calculate the gradient of the negative
log marginal likelihood with respect to the hyperparameters of the mean,
covariance, and likelihood functions.
(This is used for hyperparameter optimization).
For many models, arguments will not be needed; the quantities necessary for
inference should be provided in the constructor or generated during the
call to \code{train()}. However, an ellipses
(\code{...}) argument should always conclude the argument list.

The \code{margins()} method is used to calculate marginal (and/or partial)
effects of predictors.
The arguments may be method dependent.
An ellipses (\code{...}) argument should always conclude the argument list.

The data member \code{name} should be hard-coded within the class definition;
it is used for printing purposes (potentially in other functions). It should
be a public member so that it can be accessed directly.
}
\section{Public fields}{
\if{html}{\out{<div class="r6-fields">}}
\describe{
\item{\code{y}}{The outcomes; should be a numeric vector.
Depending on the model there may be further restrictions
on the nature of the data.
This field is usually generated automatically during object
construction and does not generally need to be interacted
with directly by the user.}

\item{\code{X}}{The predictors; should be a numeric matrix.
This field is usually generated automatically during object
construction and does not generally need to be interacted
with directly by the user.}

\item{\code{terms}}{The \code{\link[stats]{terms}} object related to the
formula provided in model construction (this is useful for
later prediction).}

\item{\code{force_all_levs}}{A logical vector of length one recording the
user's preference for dropping unused factor levels}

\item{\code{meanfun}}{The prior mean function; must be an object of class
\code{\link{MeanFunction}}}

\item{\code{covfun}}{The prior covariance function; must be an object of
class \code{\link{CovarianceFunction}}}

\item{\code{likfun}}{The likelihood function; must be an object of class
\code{\link{LikelihoodFunction}}}

\item{\code{marginal_effects}}{A list with an element for each predictor in
the model marginal effects have been requested for (via the
\code{margins()} method). Each element is a list of length two,
with an element "mean", each entry i of which gives the mean of
the distribution of the marginal effect of the predictor on the
ith observation, and an element "covariance", giving the
covariance matrix for the distribution of marginal effects of
that predictor.}

\item{\code{average_marginal_effects}}{A dataframe with a row for each
predictor in the model that marginal effects have been requested
for (via the \code{margins()} method) and columns:
"Variable", giving the name of the predictor;
"Mean", giving the mean of the average marginal effect of that
predictor;
"LB", giving the lower bound on the requested confidence
interval (see the \code{margins()} method);
and "UB", giving the upper bound on the CI.}
}
\if{html}{\out{</div>}}
}
\section{Methods}{
\subsection{Public methods}{
\itemize{
\item \href{#method-new}{\code{GPModel$new()}}
\item \href{#method-train}{\code{GPModel$train()}}
\item \href{#method-predict}{\code{GPModel$predict()}}
\item \href{#method-nlml}{\code{GPModel$nlml()}}
\item \href{#method-dnlml}{\code{GPModel$dnlml()}}
\item \href{#method-margins}{\code{GPModel$margins()}}
\item \href{#method-optimize}{\code{GPModel$optimize()}}
\item \href{#method-clone}{\code{GPModel$clone()}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-new"></a>}}
\if{latex}{\out{\hypertarget{method-new}{}}}
\subsection{Method \code{new()}}{
Create a new GPModel object
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{GPModel$new(
  formula,
  data,
  likfun = NULL,
  meanfun = NULL,
  covfun = NULL,
  optimize = FALSE,
  force_all_levs = FALSE,
  ...
)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{formula}}{A formula object giving the variable name of the
outcomes on the left-hand side and the predictors on the
right-hand side, a la \code{\link[stats]{lm}}}

\item{\code{data}}{An optional data frame where the variables in
\code{formula} are to be found. If not found there,
we search for the variables elsewhere, generally the
calling environment.}

\item{\code{likfun}}{An object inheriting from class
\code{\link{LikelihoodFunction}}, or a generator for such a
class. This is the likelihood function for the GP model.
If a generator is provided rather than an object,
an object will be created with the default value for the
hyperparameters.}

\item{\code{meanfun}}{An object inheriting from class
\code{\link{MeanFunction}}, or a generator for such a
class. This is the mean function for the GP prior (see Details).
If a generator is provided rather than an object,
an object will be created with the default value for the
hyperparameters.}

\item{\code{covfun}}{An object inheriting from class
\code{\link{CovarianceFunction}}, or a generator for such a
class.
This is the covariance function for the GP prior (see Details).
If a generator is provided rather than an object,
an object will be created with the default value for the
hyperparameters.}

\item{\code{optimize}}{A logical vector of length one; if \code{TRUE},
the hyperparameters of the mean, covariance, and likelihood
functions are optimized automatically as part of the model
construction process (see Details).
The default is \code{FALSE}, meaning the hyperparameters given
at the time of creating those objects are used for inference.}

\item{\code{force_all_levs}}{A logical vector of length one; if \code{TRUE},
unused factor levels in right-hand side variables are
\strong{not} dropped. The default is \code{FALSE}.}

\item{\code{...}}{Other arguments specific to a particular model;
see the help file for specific models for more details.
Train the GP model, providing a characterization of the posterior
of the function of interest at the input values given in the
training data.}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-train"></a>}}
\if{latex}{\out{\hypertarget{method-train}{}}}
\subsection{Method \code{train()}}{
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{GPModel$train(...)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{...}}{Additional arguments affecting the inference calculations}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-predict"></a>}}
\if{latex}{\out{\hypertarget{method-predict}{}}}
\subsection{Method \code{predict()}}{
Characterize the posterior predictive distribution of the function
of interest at new test points.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{GPModel$predict(newdata, ...)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{newdata}}{A data frame containing the data for the new test
points}

\item{\code{...}}{Additional arguments affecting the predictions produced}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-nlml"></a>}}
\if{latex}{\out{\hypertarget{method-nlml}{}}}
\subsection{Method \code{nlml()}}{
Caclulate the negative log marginal likelihood of the GP model.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{GPModel$nlml(...)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{...}}{Additional arguments affecting the calculation}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-dnlml"></a>}}
\if{latex}{\out{\hypertarget{method-dnlml}{}}}
\subsection{Method \code{dnlml()}}{
Caclulate the gradient of the negative log marginal likelihood of
the GP model with respect to the hyperparameters of the mean,
covariance, and likelihood functions.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{GPModel$dnlml(...)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{...}}{Additional arguments affecting the calculation}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-margins"></a>}}
\if{latex}{\out{\hypertarget{method-margins}{}}}
\subsection{Method \code{margins()}}{
Caclulate marginal effects of predictors.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{GPModel$margins(
  variables = NULL,
  base_categories = NULL,
  differences = NULL,
  indices = NULL,
  ci = 0.95,
  force = FALSE,
  ...
)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{variables}}{A vector specifying the variables marginal effects
are desired for; can be an integer vector, giving the column
indices of X to get effects for, or a character vector;
if NULL (the default), effects are derived for all variables.}

\item{\code{base_categories}}{If X contains contrasts,
the marginal effects will be the differences between the levels
and a base level. By default, the base level is the lowest
factor level of the contrast, but you can pass a named list to
change the base level for some or all of the variables assigned
to a contrast scheme.}

\item{\code{differences}}{A named list of 2-length numeric vectors
may be provided giving the low (first element of each vector)
and high (second element) values to calculate the effect at for
continuous variables. Any elements giving values for binary or
categorical variables are ignored (this is meaningless for
binary variables as there are only two values to choose from,
and categorical variables should be controlled with the option
base_categories).
If NULL (the default), derivatives are used for marginal
effects of continuous predictors.}

\item{\code{indices}}{A numeric vector of indices over which to average
marginal effects. If NULL (the default), all observations are
used.}

\item{\code{ci}}{A numeric vector of length one such that 0 < ci < 1
giving the width of the confidence interval for the average
marginal effects. The default is 0.95, corresponding to a
95\% confidence interval.}

\item{\code{force}}{A logical vector of length one; should marginal effects
for variables who have already had effects calculated be
re-calculated? The default is FALSE. (This is useful in case the
user has requested effects for continuous variables be
calculated as differences but would now prefer derivatives,
or vice versa).}

\item{\code{...}}{Other arguments affecting the calculations}
}
\if{html}{\out{</div>}}
}
\subsection{Details}{
The derivative of a GP is also a GP, giving us easy access to the
distribution of marginal effects of predictors. The first time this
method is called, it calculates and stores the distribution of
pointwise partial derivatives of \eqn{f} for each specified predictor
in the model (if no predictors are specified, marginal effects are
calculated for all predictors). If a predictor is binary, instead
the distribution of the difference in \eqn{f} between the predictor
taking the binary variable's highest value and its lowest value is
calculated. If a predictor is categorical, a similar calculation is
made, but comparing each of the labels except one to a baseline
label. The user may specify to use a similar calculation for two
user specified values for continuous variables rather than using
the partial derivatives (in some cases this may be more easily
interpretable). Additional calls to \code{margins()} may calculate
marginal effects for predictors whose marginal effects had not yet
been requested, but marginal effects for variables that have already
been calculated are not re-calculated unless \code{force = TRUE}.

Every time the method is called, it stores a dataframe of average
marginal effects; it calculates the mean and variance of the
marginal effect of each predictor over all observations, or over
specified indices of observations if provided.
}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-optimize"></a>}}
\if{latex}{\out{\hypertarget{method-optimize}{}}}
\subsection{Method \code{optimize()}}{
Set hyperparameters by optimizing the log marginal likelihood
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{GPModel$optimize(
  method = "CG",
  line_search = "Rasmussen",
  step0 = "rasmussen",
  ...
)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{method}}{Optimization method; default is "CG"}

\item{\code{line_search}}{Type of line search; default is "Rasmussen"}

\item{\code{step0}}{Line search value for the first step; default is
"rasmussen"}

\item{\code{...}}{Other arguments passed to \code{\link[mize]{mize}}}
}
\if{html}{\out{</div>}}
}
\subsection{Details}{
The default settings of this function should result in an
optimization routine that is very similar to that employed in the
MATLAB/Octave software GPML. For details on the optimization options,
see \code{\link[mize]{mize}}
}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-clone"></a>}}
\if{latex}{\out{\hypertarget{method-clone}{}}}
\subsection{Method \code{clone()}}{
The objects of this class are cloneable with this method.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{GPModel$clone(deep = FALSE)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{deep}}{Whether to make a deep clone.}
}
\if{html}{\out{</div>}}
}
}
}
